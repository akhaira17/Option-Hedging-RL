{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4 code:\n",
    "\n",
    "- simiulate hidden regimes + prices\n",
    "- update belief each step\n",
    "- choose hedge from state\n",
    "- compute loss\n",
    "- optimise CVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass # immutable-ish container for hyperparameters; avoids ad-hoc lists\n",
    "import torch\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class HedgeConfig:\n",
    "    T: float = 1.0              # year. keep it at one year\n",
    "    steps: int = 252            # time steps. 252 business days\n",
    "    mu: float = 0.0             # drift of S\n",
    "    r: float = 0.0              # risk-free rate\n",
    "    sigma_L: float = 0.2        # low-vol regime\n",
    "    sigma_H: float = 0.5        # high-vol regime\n",
    "    S0: float = 100.0       \n",
    "    seed: int = 1337\n",
    "    known_regime: bool = False  # debug first\n",
    "    eps: float = 0.05           # e-rectangle size\n",
    "    P_bar: torch.Tensor = None  # shape (2,2), row-stochastic\n",
    "    alpha: float = 0.95         # CVaR level\n",
    "    device: torch.device = torch.device(\"cuda\") if \\\n",
    "        torch.cuda.is_available() else (torch.device(\"mps\") \\\n",
    "            if torch.backends.mps.is_built() else torch.device(\"cpu\")) # preference for GPU, and then Macbook metal shard if using mac otherwise CPU suffices.\n",
    "    dtype: torch.dtype = torch.float32 \n",
    "    \n",
    "    @property\n",
    "    def dt(self) -> float:\n",
    "        return self.T / self.steps\n",
    "    \n",
    "    @property\n",
    "    def sqrt_dt(self) -> float:\n",
    "        return math.sqrt(self.dt)\n",
    "    \n",
    "    def rng(self) -> torch.Generator:\n",
    "        # Statefully reproducible RNG for torch ops\n",
    "        g = torch.Generator(device = 'cpu') # keep random generator on CPU because does not support mps\n",
    "        g.manual_seed(self.seed)\n",
    "        return g\n",
    "\n",
    "\n",
    "def sample_regimes(P: torch.Tensor, steps: int, start_state: int = 0, *, generator=None):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know market flips between two volatility \"moods\" (Low, High) following a Markov chain with matrix P. You do not see the mood directly. What we do see are prices and from price moves, we infer a belief $q_t$ = [Pr(L), Pr(H)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi = tensor([0.6667, 0.3333], dtype=torch.float64)\n",
      "sum = 1.0\n",
      "fixed point OK: True\n"
     ]
    }
   ],
   "source": [
    "# take yesterday belief and flow it through the Markov chain\n",
    "### FILTER.PY\n",
    "\n",
    "def hmm_predict(q_prev: torch.Tensor, P:torch.Tensor) -> torch.Tensor:\n",
    "    assert q_prev.shape == (2,); assert P.shape == (2,2)\n",
    "    q_pred = P.T @ q_prev \n",
    "    q_pred = q_pred.clamp(0,1) # ensure we do not get out of bound values\n",
    "    q_pred /= q_pred.sum() # normalise just in case\n",
    "    return q_pred\n",
    "\n",
    "def hmm_update(q_pred: torch.Tensor, x: torch.Tensor,\n",
    "               mu: float, sigma_L: float, sigma_H: float, dt: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    q_pred: (2,), prior after predict\n",
    "    x: sxcalar log-return at this step\n",
    "    returns: q_post: (2,)\n",
    "    \"\"\"\n",
    "    mL = mu * 0.5 * (sigma_L**2)\n",
    "    mH = mu - 0.5 * (sigma_H**2)\n",
    "    vL = sigma_L**2\n",
    "    vH = sigma_H**2\n",
    "    \n",
    "    # log-likelihoods for Normal(x; mean=m*dt, var=v*dt)\n",
    "    # logN = -0.5*log(2*pi*v*dt) - 0.5*((x - m*dt)^2)/(v*dt)\n",
    "    const = -0.5 * math.log(2*math.pi)\n",
    "    loglik_L = const - 0.5*math.log(vL*dt) - 0.5*((x - mL*dt)**2)/(vL*dt)\n",
    "    loglik_H = const - 0.5*math.log(vH*dt) - 0.5*((x - mH*dt)**2)/(vH*dt)\n",
    "\n",
    "    log_qpred = torch.log(q_pred.clamp_min(1e-32))\n",
    "    log_post_unnorm = torch.stack([loglik_L, loglik_H]) + log_qpred\n",
    "    \n",
    "    # softmax = exp - logsumexp\n",
    "    maxlog = log_post_unnorm.max()\n",
    "    exps = torch.exp(log_post_unnorm - maxlog)\n",
    "    q_post = exps / exps.sum()\n",
    "\n",
    "    # guardrails\n",
    "    q_post = q_post.clamp(0, 1)\n",
    "    s = q_post.sum()\n",
    "    return q_post if s > 0 else torch.tensor([0.5, 0.5], dtype=q_pred.dtype, device=q_pred.device)\n",
    "\n",
    "def stationary_2state(P: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    P: (2,2) row-stochastic transition matrix\n",
    "    return: pi (2,) with pi.sum()==1 and P.T @ pi == pi (within tolerance)\n",
    "    \"\"\"\n",
    "    p_LH = P[0,1]\n",
    "    p_HL = P[1,0]\n",
    "    # unnormalised weights \n",
    "    wL = p_HL\n",
    "    wH = p_LH\n",
    "    # guardrail \n",
    "    denom = wL + wH\n",
    "    if float(denom) <= 0.0:\n",
    "        return torch.tensor([0.5, 0.5], dtype=P.dtype, device=P.device)\n",
    "    \n",
    "    w = torch.stack([wL, wH]).clamp_min(0)\n",
    "    pi = w/w.sum()\n",
    "    \n",
    "    return pi\n",
    "    \n",
    "\n",
    "    \n",
    "P = torch.tensor([[0.9, 0.1],\n",
    "                  [0.2, 0.8]], dtype=torch.float64)\n",
    "pi = stationary_2state(P)\n",
    "print(\"pi =\", pi)\n",
    "print(\"sum =\", pi.sum().item())\n",
    "print(\"fixed point OK:\", torch.allclose(P.T @ pi, pi, atol=1e-10))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means in the long run, the chain spends about 2/3 of time in Low and 1/3 of time High."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_high_vol_returns(steps: int, mu: float, sigma_H: float, dt: float, *, generator=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a length 'steps' tensor of log-returns generated under the High-vol regime\n",
    "    \"\"\"\n",
    "    mH = mu - 0.5 * (sigma_H ** 2)\n",
    "    \n",
    "    # sample standard normals Z_t\n",
    "    Z = torch.randn(steps, generator=generator)\n",
    "    \n",
    "    # Build log-returns\n",
    "    x = mH * dt + sigma_H * math.sqrt(dt) * Z\n",
    "    return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0009849938796833158 0.03779744729399681\n"
     ]
    }
   ],
   "source": [
    "steps = 50_000\n",
    "mu, sigma_H, dt = 0.0, 0.6, 1/252\n",
    "g = torch.Generator().manual_seed(0)\n",
    "xs = simulate_high_vol_returns(steps, mu, sigma_H, dt, generator=g)\n",
    "print(xs.mean().item(), xs.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1161, 0.0407, 0.2330, 1.0000, 0.9976, 0.3976, 1.0000, 0.4085, 0.4469,\n",
      "        1.0000])\n",
      "0.9980629086494446\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def belief_demo(\n",
    "    P: torch.Tensor,\n",
    "    q0: torch.Tensor,\n",
    "    xs: torch.Tensor,\n",
    "    mu: float,\n",
    "    sigma_L: float,\n",
    "    sigma_H: float,\n",
    "    dt: float,\n",
    "    *,\n",
    "    burn_in: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Run a predict→update belief filter over a sequence of log-returns.\n",
    "\n",
    "    Returns a tensor of the posterior High-regime beliefs q_t(H) at each step,\n",
    "    optionally discarding the first `burn_in` steps from the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure everything lives on the same device/dtype\n",
    "    xs = xs.reshape(-1)  # ensure 1D\n",
    "    dtype, device = xs.dtype, xs.device\n",
    "    P = P.to(device=device, dtype=dtype)\n",
    "    q = q0.to(device=device, dtype=dtype)\n",
    "\n",
    "    # storage for the High-belief each step\n",
    "    qH_series = []\n",
    "\n",
    "    # Core loop: for each observed log-return x_t\n",
    "    for x in xs:\n",
    "        # Predict: push yesterday's belief through the Markov chain\n",
    "        #      q_{t|t-1} = P^T @ q_{t-1}\n",
    "        q = hmm_predict(q, P)\n",
    "\n",
    "        # Update; reweight with the likelihood of the observed return\n",
    "        #      q_t ∝ ℓ(x_t) ⊙ q_{t|t-1}  (done in log-space inside hmm_update)\n",
    "        q = hmm_update(q, x, mu, sigma_L, sigma_H, dt)\n",
    "\n",
    "        # Record the High component for plotting / inspection\n",
    "        qH_series.append(q[1])\n",
    "\n",
    "    qH_series = torch.stack(qH_series)\n",
    "\n",
    "    # early steps can be prior-driven; trim if requested\n",
    "    if burn_in > 0:\n",
    "        burn_in = min(burn_in, qH_series.numel())\n",
    "        qH_series = qH_series[burn_in:]\n",
    "\n",
    "    return qH_series\n",
    "\n",
    "P  = torch.tensor([[0.9, 0.1],\n",
    "                   [0.2, 0.8]], dtype=torch.float64)\n",
    "pi = stationary_2state(P)\n",
    "\n",
    "cfg = HedgeConfig()\n",
    "g   = cfg.rng()\n",
    "\n",
    "xs  = simulate_high_vol_returns(steps=60, mu=0.0, sigma_H=0.6, dt=cfg.dt, generator=g)\n",
    "qHs = belief_demo(P, pi, xs, mu=0.0, sigma_L=0.10, sigma_H=0.60, dt=cfg.dt, burn_in=0)\n",
    "\n",
    "print(qHs[:10])        # should start near ~0.33 (the stationary H weight)\n",
    "print(qHs[-1].item())  # should be well above ~0.6–0.8 on a high-vol path (noise is fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
